# Generated by ffi_gen. Please do not change this file by hand.

require 'ffi'

module RbKafka
  extend FFI::Library
  ffi_lib "librdkafka.1.dylib"
  
  def self.attach_function(name, *_)
    begin; super; rescue FFI::NotFoundError => e
      (class << self; self; end).class_eval { define_method(name) { |*_| raise e } }
    end
  end
  
  RD_KAFKA_VERSION = 0x00090100
  
  RD_KAFKA_DEBUG_CONTEXTS = "all,generic,broker,topic,metadata,producer,queue,msg,protocol,cgrp,security,fetch"
  
  RD_KAFKA_OFFSET_BEGINNING = -2
  
  RD_KAFKA_OFFSET_END = -1
  
  RD_KAFKA_OFFSET_STORED = -1000
  
  RD_KAFKA_OFFSET_INVALID = -1001
  
  RD_KAFKA_OFFSET_TAIL_BASE = -2000
  
  RD_KAFKA_MSG_F_FREE = 0x1
  
  RD_KAFKA_MSG_F_COPY = 0x2
  
  # @brief Returns the librdkafka version as integer.
  # 
  # @returns Version integer.
  # 
  # @sa See RD_KAFKA_VERSION for how to parse the integer format.
  # @sa Use rd_kafka_version_str() to retreive the version as a string.
  # 
  # @method rd_kafka_version()
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_version, :rd_kafka_version, [], :int
  
  # @brief Returns the librdkafka version as string.
  # 
  # @returns Version string
  # 
  # @method rd_kafka_version_str()
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_version_str, :rd_kafka_version_str, [], :string
  
  # @enum rd_kafka_type_t
  # 
  # @brief rd_kafka_t handle type.
  # 
  # @sa rd_kafka_new()
  # 
  # <em>This entry is only for documentation and no real method. The FFI::Enum can be accessed via #enum_type(:rd_kafka_type_t).</em>
  # 
  # === Options:
  # :producer ::
  #   
  # :consumer ::
  #   < Producer client
  # 
  # @method _enum_rd_kafka_type_t_
  # @return [Symbol]
  # @scope class
  enum :rd_kafka_type_t, [
    :producer, 0,
    :consumer, 1
  ]
  
  # @brief Retrieve supported debug contexts for use with the \c \"debug\"
  #        configuration property. (runtime)
  # 
  # @returns Comma-separated list of available debugging contexts.
  # 
  # @method rd_kafka_get_debug_contexts()
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_get_debug_contexts, :rd_kafka_get_debug_contexts, [], :string
  
  # Private types to provide ABI compatibility
  class RdKafkaS < FFI::Struct
    layout :dummy, :char
  end
  
  # (Not documented)
  class RdKafkaTopicS < FFI::Struct
    layout :dummy, :pointer
  end
  
  # (Not documented)
  class RdKafkaConfS < FFI::Struct
    layout :dummy, :char
  end
  
  # (Not documented)
  class RdKafkaTopicConfS < FFI::Struct
    layout :dummy, :char
  end
  
  # (Not documented)
  class RdKafkaQueueS < FFI::Struct
    layout :dummy, :char
  end
  
  # @enum rd_kafka_resp_err_t
  # @brief Error codes.
  # 
  # The negative error codes delimited by two underscores
  # (\c RD_KAFKA_RESP_ERR__..) denotes errors internal to librdkafka and are
  # displayed as \c \"Local: \<error string..\>\", while the error codes
  # delimited by a single underscore (\c RD_KAFKA_RESP_ERR_..) denote broker
  # errors and are displayed as \c \"Broker: \<error string..\>\".
  # 
  # @sa Use rd_kafka_err2str() to translate an error code a human readable string
  # 
  # <em>This entry is only for documentation and no real method. The FFI::Enum can be accessed via #enum_type(:rd_kafka_resp_err_t).</em>
  # 
  # === Options:
  # :begin_ ::
  #   Begin internal error codes
  # :bad_msg ::
  #   Received message is incorrect
  # :bad_compression ::
  #   Bad/unknown compression
  # :destroy ::
  #   Broker is going away
  # :fail ::
  #   Generic failure
  # :transport ::
  #   Broker transport failure
  # :crit_sys_resource ::
  #   Critical system resource
  # :resolve ::
  #   Failed to resolve broker
  # :msg_timed_out ::
  #   Produced message timed out
  # :partition_eof ::
  #   Reached the end of the topic+partition queue on
  #   the broker. Not really an error.
  # :unknown_partition ::
  #   Permanent: Partition does not exist in cluster.
  # :fs ::
  #   File or filesystem error
  # :unknown_topic ::
  #   Permanent: Topic does not exist in cluster.
  # :all_brokers_down ::
  #   All broker connections are down.
  # :invalid_arg ::
  #   Invalid argument, or invalid configuration
  # :timed_out ::
  #   Operation timed out
  # :queue_full ::
  #   Queue is full
  # :isr_insuff ::
  #   ISR count < required.acks
  # :node_update ::
  #   Broker node update
  # :ssl ::
  #   SSL error
  # :wait_coord ::
  #   Waiting for coordinator to become available.
  # :unknown_group ::
  #   Unknown client group
  # :in_progress ::
  #   Operation in progress
  # :prev_in_progress ::
  #   Previous operation in progress, wait for it to finish.
  # :existing_subscription ::
  #   This operation would interfere with an existing subscription
  # :assign_partitions ::
  #   Assigned partitions (rebalance_cb)
  # :revoke_partitions ::
  #   Revoked partitions (rebalance_cb)
  # :conflict ::
  #   Conflicting use
  # :state ::
  #   Wrong state
  # :unknown_protocol ::
  #   Unknown protocol
  # :not_implemented ::
  #   Not implemented
  # :authentication ::
  #   Authentication failure
  # :no_offset ::
  #   No stored offset
  # :end_ ::
  #   End internal error codes
  # :unknown ::
  #   Unknown broker error
  # :no_error ::
  #   Success
  # :offset_out_of_range ::
  #   Offset out of range
  # :invalid_msg ::
  #   Invalid message
  # :unknown_topic_or_part ::
  #   Unknown topic or partition
  # :invalid_msg_size ::
  #   Invalid message size
  # :leader_not_available ::
  #   Leader not available
  # :not_leader_for_partition ::
  #   Not leader for partition
  # :request_timed_out ::
  #   Request timed out
  # :broker_not_available ::
  #   Broker not available
  # :replica_not_available ::
  #   Replica not available
  # :msg_size_too_large ::
  #   Message size too large
  # :stale_ctrl_epoch ::
  #   StaleControllerEpochCode
  # :offset_metadata_too_large ::
  #   Offset metadata string too large
  # :network_exception ::
  #   Broker disconnected before response received
  # :group_load_in_progress ::
  #   Group coordinator load in progress
  # :group_coordinator_not_available ::
  #   Group coordinator not available
  # :not_coordinator_for_group ::
  #   Not coordinator for group
  # :topic_exception ::
  #   Invalid topic
  # :record_list_too_large ::
  #   Message batch larger than configured server segment size
  # :not_enough_replicas ::
  #   Not enough in-sync replicas
  # :not_enough_replicas_after_append ::
  #   Message(s) written to insufficient number of in-sync replicas
  # :invalid_required_acks ::
  #   Invalid required acks value
  # :illegal_generation ::
  #   Specified group generation id is not valid
  # :inconsistent_group_protocol ::
  #   Inconsistent group protocol
  # :invalid_group_id ::
  #   Invalid group.id
  # :unknown_member_id ::
  #   Unknown member
  # :invalid_session_timeout ::
  #   Invalid session timeout
  # :rebalance_in_progress ::
  #   Group rebalance in progress
  # :invalid_commit_offset_size ::
  #   Commit offset data size is not valid
  # :topic_authorization_failed ::
  #   Topic authorization failed
  # :group_authorization_failed ::
  #   Group authorization failed
  # :cluster_authorization_failed ::
  #   Cluster authorization failed
  # :end_ ::
  #   
  # 
  # @method _enum_rd_kafka_resp_err_t_
  # @return [Symbol]
  # @scope class
  enum :rd_kafka_resp_err_t, [
    :begin_, -200,
    :bad_msg, -199,
    :bad_compression, -198,
    :destroy, -197,
    :fail, -196,
    :transport, -195,
    :crit_sys_resource, -194,
    :resolve, -193,
    :msg_timed_out, -192,
    :partition_eof, -191,
    :unknown_partition, -190,
    :fs, -189,
    :unknown_topic, -188,
    :all_brokers_down, -187,
    :invalid_arg, -186,
    :timed_out, -185,
    :queue_full, -184,
    :isr_insuff, -183,
    :node_update, -182,
    :ssl, -181,
    :wait_coord, -180,
    :unknown_group, -179,
    :in_progress, -178,
    :prev_in_progress, -177,
    :existing_subscription, -176,
    :assign_partitions, -175,
    :revoke_partitions, -174,
    :conflict, -173,
    :state, -172,
    :unknown_protocol, -171,
    :not_implemented, -170,
    :authentication, -169,
    :no_offset, -168,
    :end_, -100,
    :unknown, -1,
    :no_error, 0,
    :offset_out_of_range, 1,
    :invalid_msg, 2,
    :unknown_topic_or_part, 3,
    :invalid_msg_size, 4,
    :leader_not_available, 5,
    :not_leader_for_partition, 6,
    :request_timed_out, 7,
    :broker_not_available, 8,
    :replica_not_available, 9,
    :msg_size_too_large, 10,
    :stale_ctrl_epoch, 11,
    :offset_metadata_too_large, 12,
    :network_exception, 13,
    :group_load_in_progress, 14,
    :group_coordinator_not_available, 15,
    :not_coordinator_for_group, 16,
    :topic_exception, 17,
    :record_list_too_large, 18,
    :not_enough_replicas, 19,
    :not_enough_replicas_after_append, 20,
    :invalid_required_acks, 21,
    :illegal_generation, 22,
    :inconsistent_group_protocol, 23,
    :invalid_group_id, 24,
    :unknown_member_id, 25,
    :invalid_session_timeout, 26,
    :rebalance_in_progress, 27,
    :invalid_commit_offset_size, 28,
    :topic_authorization_failed, 29,
    :group_authorization_failed, 30,
    :cluster_authorization_failed, 31,
  ]
  
  # @brief Error code value, name and description.
  #        Typically for use with language bindings to automatically expose
  #        the full set of librdkafka error codes.
  # 
  # = Fields:
  # :code ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Error code
  # :name ::
  #   (String) < Error name, same as code enum sans prefix
  # :desc ::
  #   (String) < Human readable error description.
  class RdKafkaErrDesc < FFI::Struct
    layout :code, :rd_kafka_resp_err_t,
           :name, :string,
           :desc, :string
  end
  
  # @brief Returns the full list of error codes.
  # 
  # @method rd_kafka_get_err_descs(errdescs, cntp)
  # @param [FFI::Pointer(**RdKafkaErrDesc)] errdescs 
  # @param [FFI::Pointer(*SizeT)] cntp 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_get_err_descs, :rd_kafka_get_err_descs, [:pointer, :pointer], :void
  
  # @brief Returns a human readable representation of a kafka error.
  # 
  # @param err Error code to translate
  # 
  # @method rd_kafka_err2str(err)
  # @param [Symbol from _enum_rd_kafka_resp_err_t_] err 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_err2str, :rd_kafka_err2str, [:rd_kafka_resp_err_t], :string
  
  # @brief Returns the error code name (enum name).
  # 
  # @param err Error code to translate
  # 
  # @method rd_kafka_err2name(err)
  # @param [Symbol from _enum_rd_kafka_resp_err_t_] err 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_err2name, :rd_kafka_err2name, [:rd_kafka_resp_err_t], :string
  
  # @brief Converts the system errno value \p errnox to a rd_kafka_resp_err_t
  #        error code upon failure from the following functions:
  #  - rd_kafka_topic_new()
  #  - rd_kafka_consume_start()
  #  - rd_kafka_consume_stop()
  #  - rd_kafka_consume()
  #  - rd_kafka_consume_batch()
  #  - rd_kafka_consume_callback()
  #  - rd_kafka_produce()
  # 
  # @param errnox  System errno value to convert
  # 
  # @returns Appropriate error code for \p errnox
  # 
  # @method rd_kafka_errno2err(errnox)
  # @param [Integer] errnox 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_errno2err, :rd_kafka_errno2err, [:int], :rd_kafka_resp_err_t
  
  # @brief Returns the thread-local system errno
  # 
  # On most platforms this is the same as \p errno but in case of different
  # runtimes between library and application (e.g., Windows static DLLs)
  # this provides a means for expsing the errno librdkafka uses.
  # 
  # @remark The value is local to the current calling thread.
  # 
  # @method rd_kafka_errno()
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_errno, :rd_kafka_errno, [], :int
  
  # @brief Generic place holder for a specific Topic+Partition.
  # 
  # @sa rd_kafka_topic_partition_list_new()
  # 
  # = Fields:
  # :topic ::
  #   (String) < Topic name
  # :partition ::
  #   (Integer) < Partition
  # :offset ::
  #   (Integer) < Offset
  # :metadata ::
  #   (FFI::Pointer(*Void)) < Metadata
  # :metadata_size ::
  #   (Integer) < Metadata size
  # :opaque ::
  #   (FFI::Pointer(*Void)) < Application opaque
  # :err ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Error code, depending on use.
  # :private ::
  #   (FFI::Pointer(*Void)) < INTERNAL USE ONLY,
  #     INITIALIZE TO ZERO, DO NOT TOUCH
  class RdKafkaTopicPartitionS < FFI::Struct
    layout :topic, :string,
           :partition, :int,
           :offset, :long_long,
           :metadata, :pointer,
           :metadata_size, :ulong,
           :opaque, :pointer,
           :err, :rd_kafka_resp_err_t,
           :private, :pointer
  end
  
  # @brief A growable list of Topic+Partitions.
  # 
  # = Fields:
  # :cnt ::
  #   (Integer) < Current number of elements
  # :size ::
  #   (Integer) < Current allocated size
  # :elems ::
  #   (RdKafkaTopicPartitionS) < Element array()
  class RdKafkaTopicPartitionListS < FFI::Struct
    layout :cnt, :int,
           :size, :int,
           :elems, RdKafkaTopicPartitionS
  end
  
  # @brief Create a new list/vector Topic+Partition container.
  # 
  # @param size  Initial allocated size used when the expected number of
  #              elements is known or can be estimated.
  #              Avoids reallocation and possibly relocation of the
  #              elems array.
  # 
  # @returns A newly allocated Topic+Partition list.
  # 
  # @remark Use rd_kafka_topic_partition_list_destroy() to free all resources
  #         in use by a list and the list itself.
  # @sa     rd_kafka_topic_partition_list_add()
  # 
  # @method rd_kafka_topic_partition_list_new(size)
  # @param [Integer] size 
  # @return [RdKafkaTopicPartitionListS] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_new, :rd_kafka_topic_partition_list_new, [:int], RdKafkaTopicPartitionListS
  
  # @brief Free all resources used by the list and the list itself.
  # 
  # @method rd_kafka_topic_partition_list_destroy(rkparlist)
  # @param [RdKafkaTopicPartitionListS] rkparlist 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_destroy, :rd_kafka_topic_partition_list_destroy, [RdKafkaTopicPartitionListS], :void
  
  # @brief Add topic+partition to list
  # 
  # @param rktparlist List to extend
  # @param topic      Topic name (copied)
  # @param partition  Partition id
  # 
  # @returns The object which can be used to fill in additionals fields.
  # 
  # @method rd_kafka_topic_partition_list_add(rktparlist, topic, partition)
  # @param [RdKafkaTopicPartitionListS] rktparlist 
  # @param [String] topic 
  # @param [Integer] partition 
  # @return [RdKafkaTopicPartitionS] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_add, :rd_kafka_topic_partition_list_add, [RdKafkaTopicPartitionListS, :string, :int], RdKafkaTopicPartitionS
  
  # @brief Add range of partitions from \p start to \p stop inclusive.
  # 
  # @param rktparlist List to extend
  # @param topic      Topic name (copied)
  # @param start      Start partition of range
  # @param stop       Last partition of range (inclusive)
  # 
  # @method rd_kafka_topic_partition_list_add_range(rktparlist, topic, start, stop)
  # @param [RdKafkaTopicPartitionListS] rktparlist 
  # @param [String] topic 
  # @param [Integer] start 
  # @param [Integer] stop 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_add_range, :rd_kafka_topic_partition_list_add_range, [RdKafkaTopicPartitionListS, :string, :int, :int], :void
  
  # @brief Make a copy of an existing list.
  # 
  # @param src   The existing list to copy.
  # 
  # @returns A new list fully populated to be identical to \p src
  # 
  # @method rd_kafka_topic_partition_list_copy(src)
  # @param [RdKafkaTopicPartitionListS] src 
  # @return [RdKafkaTopicPartitionListS] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_copy, :rd_kafka_topic_partition_list_copy, [RdKafkaTopicPartitionListS], RdKafkaTopicPartitionListS
  
  # @brief Set offset to \p offset for \p topic and \p partition
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or
  #          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if \p partition was not found
  #          in the list.
  # 
  # @method rd_kafka_topic_partition_list_set_offset(rktparlist, topic, partition, offset)
  # @param [RdKafkaTopicPartitionListS] rktparlist 
  # @param [String] topic 
  # @param [Integer] partition 
  # @param [Integer] offset 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_set_offset, :rd_kafka_topic_partition_list_set_offset, [RdKafkaTopicPartitionListS, :string, :int, :long_long], :rd_kafka_resp_err_t
  
  # @brief Find element by \p topic and \p partition.
  # 
  # @returns a pointer to the first matching element, or NULL if not found.
  # 
  # @method rd_kafka_topic_partition_list_find(rktparlist, topic, partition)
  # @param [RdKafkaTopicPartitionListS] rktparlist 
  # @param [String] topic 
  # @param [Integer] partition 
  # @return [RdKafkaTopicPartitionS] 
  # @scope class
  attach_function :rd_kafka_topic_partition_list_find, :rd_kafka_topic_partition_list_find, [RdKafkaTopicPartitionListS, :string, :int], RdKafkaTopicPartitionS
  
  # @brief A Kafka message as returned by the \c rd_kafka_consume*() family
  #        of functions.
  # 
  # This object has two purposes:
  #  - provide the application with a consumed message. (\c err == 0)
  #  - report per-topic+partition consumer errors (\c err != 0)
  # 
  # The application must check \c err to decide what action to take.
  # 
  # When the application is finished with a message it must call
  # rd_kafka_message_destroy() unless otherwise noted.
  # 
  # = Fields:
  # :err ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Non-zero for error signaling.
  # :rkt ::
  #   (RdKafkaTopicS) < Topic
  # :partition ::
  #   (Integer) < Partition
  # :payload ::
  #   (FFI::Pointer(*Void)) < Depends on the value of \c err :
  #   - \c err==0: Message payload.
  #   - \c err!=0: Error string
  # :len ::
  #   (Integer) < Depends on the value of \c err :
  #   - \c err==0: Message payload length
  #   - \c err!=0: Error string length
  # :key ::
  #   (FFI::Pointer(*Void)) < Depends on the value of \c err :
  #   - \c err==0: Optional message key
  # :key_len ::
  #   (Integer) < Depends on the value of \c err :
  #   - \c err==0: Optional message key length
  # :offset ::
  #   (Integer) < Consume:
  #   - Message offset (or offset for error
  #     if \c err!=0 if applicable).
  #   - dr_msg_cb:
  #     Message offset assigned by broker.
  #     If \c produce.offset.report is set then
  #     each message will have this field set,
  #     otherwise only the last message in
  #     each produced internal batch will
  #     have this field set, otherwise 0.
  # :private ::
  #   (FFI::Pointer(*Void)) < Consume:
  #    - rdkafka private pointer: DO NOT MODIFY
  #    - dr_msg_cb:
  #      msg_opaque from produce() call
  class RdKafkaMessageS < FFI::Struct
    layout :err, :rd_kafka_resp_err_t,
           :rkt, RdKafkaTopicS,
           :partition, :int,
           :payload, :pointer,
           :len, :ulong,
           :key, :pointer,
           :key_len, :ulong,
           :offset, :long_long,
           :private, :pointer
  end
  
  # @brief Frees resources for \p rkmessage and hands ownership back to rdkafka.
  # 
  # @method rd_kafka_message_destroy(rkmessage)
  # @param [RdKafkaMessageS] rkmessage 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_message_destroy, :rd_kafka_message_destroy, [RdKafkaMessageS], :void
  
  # @brief Returns the error string for an errored rd_kafka_message_t or NULL if
  #        there was no error.
  # 
  # @method rd_kafka_message_errstr(rkmessage)
  # @param [RdKafkaMessageS] rkmessage 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_message_errstr, :rd_kafka_message_errstr, [RdKafkaMessageS], :string
  
  # @enum rd_kafka_conf_res_t
  # @brief Configuration result type
  # 
  # <em>This entry is only for documentation and no real method. The FFI::Enum can be accessed via #enum_type(:rd_kafka_conf_res_t).</em>
  # 
  # === Options:
  # :unknown ::
  #   
  # :invalid ::
  #   < Unknown configuration name.
  # :ok ::
  #   < Invalid configuration value.
  # 
  # @method _enum_rd_kafka_conf_res_t_
  # @return [Symbol]
  # @scope class
  enum :rd_kafka_conf_res_t, [
    :unknown, -2,
    :invalid, -1,
    :ok, 0
  ]
  
  # @brief Create configuration object.
  # 
  # When providing your own configuration to the \c rd_kafka_*_new_*() calls
  # the rd_kafka_conf_t objects needs to be created with this function
  # which will set up the defaults.
  # I.e.:
  # @code
  #   rd_kafka_conf_t *myconf;
  #   rd_kafka_conf_res_t res;
  # 
  #   myconf = rd_kafka_conf_new();
  #   res = rd_kafka_conf_set(myconf, "socket.timeout.ms", "600",
  #                           errstr, sizeof(errstr));
  #   if (res != RD_KAFKA_CONF_OK)
  #      die("%s\n", errstr);
  #   
  #   rk = rd_kafka_new(..., myconf);
  # @endcode
  # 
  # Please see CONFIGURATION.md for the default settings or use
  # rd_kafka_conf_properties_show() to provide the information at runtime.
  # 
  # The properties are identical to the Apache Kafka configuration properties
  # whenever possible.
  # 
  # @returns A new rd_kafka_conf_t object with defaults set.
  # 
  # @sa rd_kafka_conf_set(), rd_kafka_conf_destroy()
  # 
  # @method rd_kafka_conf_new()
  # @return [RdKafkaConfS] 
  # @scope class
  attach_function :rd_kafka_conf_new, :rd_kafka_conf_new, [], RdKafkaConfS
  
  # @brief Destroys a conf object.
  # 
  # @method rd_kafka_conf_destroy(conf)
  # @param [RdKafkaConfS] conf 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_destroy, :rd_kafka_conf_destroy, [RdKafkaConfS], :void
  
  # @brief Creates a copy/duplicate of configuration object \p conf
  # 
  # @method rd_kafka_conf_dup(conf)
  # @param [RdKafkaConfS] conf 
  # @return [RdKafkaConfS] 
  # @scope class
  attach_function :rd_kafka_conf_dup, :rd_kafka_conf_dup, [RdKafkaConfS], RdKafkaConfS
  
  # @brief Sets a configuration property.
  # 
  # \p must have been previously created with rd_kafka_conf_new().
  # 
  # Returns \c rd_kafka_conf_res_t to indicate success or failure.
  # In case of failure \p errstr is updated to contain a human readable
  # error string.
  # 
  # @method rd_kafka_conf_set(conf, name, value, errstr, errstr_size)
  # @param [RdKafkaConfS] conf 
  # @param [String] name 
  # @param [String] value 
  # @param [String] errstr 
  # @param [Integer] errstr_size 
  # @return [Symbol from _enum_rd_kafka_conf_res_t_] 
  # @scope class
  attach_function :rd_kafka_conf_set, :rd_kafka_conf_set, [RdKafkaConfS, :string, :string, :string, :ulong], :rd_kafka_conf_res_t
  
  # @deprecated See rd_kafka_conf_set_dr_msg_cb()
  # 
  # @method rd_kafka_conf_set_dr_cb(conf, dr_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] dr_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_dr_cb, :rd_kafka_conf_set_dr_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief \b Producer: Set delivery report callback in provided \p conf object.
  # 
  # The delivery report callback will be called once for each message
  # accepted by rd_kafka_produce() (et.al) with \p err set to indicate
  # the result of the produce request.
  # 
  # The callback is called when a message is succesfully produced or
  # if librdkafka encountered a permanent failure, or the retry counter for
  # temporary errors has been exhausted.
  # 
  # An application must call rd_kafka_poll() at regular intervals to
  # serve queued delivery report callbacks.
  # 
  # @method rd_kafka_conf_set_dr_msg_cb(conf, dr_msg_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] dr_msg_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_dr_msg_cb, :rd_kafka_conf_set_dr_msg_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief \b Consumer: Set consume callback for use with rd_kafka_consumer_poll()
  # 
  # @method rd_kafka_conf_set_consume_cb(conf, consume_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] consume_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_consume_cb, :rd_kafka_conf_set_consume_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief \b Consumer: Set rebalance callback for use with
  #                     coordinated consumer group balancing.
  # 
  # The \p err field is set to either RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
  # or RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS and 'partitions'
  # contains the full partition set that was either assigned or revoked.
  # 
  # Registering a \p rebalance_cb turns off librdkafka's automatic
  # partition assignment/revocation and instead delegates that responsibility
  # to the application's \p rebalance_cb.
  # 
  # The rebalance callback is responsible for updating librdkafka's
  # assignment set based on the two events: RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
  # and RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS but should also be able to handle
  # arbitrary rebalancing failures where \p err is neither of those.
  # @remark In this latter case (arbitrary error), the application must
  #         call rd_kafka_assign(rk, NULL) to synchronize state.
  # 
  # Without a rebalance callback this is done automatically by librdkafka
  # but registering a rebalance callback gives the application flexibility
  # in performing other operations along with the assinging/revocation,
  # such as fetching offsets from an alternate location (on assign)
  # or manually committing offsets (on revoke).
  # 
  # The following example show's the application's responsibilities:
  # @code
  #    static void rebalance_cb (rd_kafka_t *rk, rd_kafka_resp_err_t err,
  #                              rd_kafka_topic_partition_list_t *partitions,
  #                              void *opaque) {
  # 
  #        switch (err)
  #        {
  #          case RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS:
  #             // application may load offets from arbitrary external
  #             // storage here and update \p partitions
  # 
  #             rd_kafka_assign(rk, partitions);
  #             break;
  # 
  #          case RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS:
  #             if (manual_commits) // Optional explicit manual commit
  #                 rd_kafka_commit(rk, partitions, 0); // sync commit
  # 
  #             rd_kafka_assign(rk, NULL);
  #             break;
  # 
  #          default:
  #             handle_unlikely_error(err);
  #             rd_kafka_assign(rk, NULL); // sync state
  #             break;
  #         }
  #    }
  # @endcode
  # 
  # @method rd_kafka_conf_set_rebalance_cb(conf, rebalance_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] rebalance_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_rebalance_cb, :rd_kafka_conf_set_rebalance_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief \b Consumer: Set offset commit callback for use with consumer groups.
  # 
  # The results of automatic or manual offset commits will be scheduled
  # for this callback and is served by rd_kafka_consumer_poll().
  # 
  # If no partitions had valid offsets to commit this callback will be called
  # with \p err == RD_KAFKA_RESP_ERR__NO_OFFSET which is not to be considered
  # an error.
  # 
  # The \p offsets list contains per-partition information:
  #   - \c offset: committed offset (attempted)
  #   - \c err:    commit error
  # 
  # @method rd_kafka_conf_set_offset_commit_cb(conf, offset_commit_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] offset_commit_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_offset_commit_cb, :rd_kafka_conf_set_offset_commit_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set error callback in provided conf object.
  # 
  # The error callback is used by librdkafka to signal critical errors
  # back to the application.
  # 
  # If no \p error_cb is registered then the errors will be logged instead.
  # 
  # @method rd_kafka_conf_set_error_cb(conf, error_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] error_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_error_cb, :rd_kafka_conf_set_error_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set throttle callback.
  # 
  # The throttle callback is used in conjunction with
  # \c quota.support.enable=true to forward broker throttle times to the
  # application for Produce and Fetch (consume) requests.
  # 
  # Callbacks are triggered whenever a non-zero throttle time is returned by
  # the broker, or when the throttle time drops back to zero.
  # 
  # An application must call rd_kafka_poll() or rd_kafka_consumer_poll() at
  # regular intervals to serve queued callbacks.
  # 
  # @method rd_kafka_conf_set_throttle_cb(conf, throttle_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] throttle_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_throttle_cb, :rd_kafka_conf_set_throttle_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set logger callback.
  # 
  # The default is to print to stderr, but a syslog logger is also available,
  # see rd_kafka_log_print and rd_kafka_log_syslog for the builtin alternatives.
  # Alternatively the application may provide its own logger callback.
  # Or pass \p func as NULL to disable logging.
  # 
  # This is the configuration alternative to the deprecated rd_kafka_set_logger()
  # 
  # @method rd_kafka_conf_set_log_cb(conf, log_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] log_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_log_cb, :rd_kafka_conf_set_log_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set statistics callback in provided conf object.
  # 
  # The statistics callback is triggered from rd_kafka_poll() every
  # \c statistics.interval.ms (needs to be configured separately).
  # Function arguments:
  #   - \p rk - Kafka handle
  #   - \p json - String containing the statistics data in JSON format
  #   - \p json_len - Length of \p json string.
  #   - \p opaque - Application-provided opaque.
  # 
  # If the application wishes to hold on to the \p json pointer and free
  # it at a later time it must return 1 from the \p stats_cb.
  # If the application returns 0 from the \p stats_cb then librdkafka
  # will immediately free the \p json pointer.
  # 
  # @method rd_kafka_conf_set_stats_cb(conf, stats_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] stats_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_stats_cb, :rd_kafka_conf_set_stats_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set socket callback.
  # 
  # The socket callback is responsible for opening a socket
  # according to the supplied \p domain, \p type and \p protocol.
  # The socket shall be created with \c CLOEXEC set in a racefree fashion, if
  # possible.
  # 
  # Default:
  #  - on linux: racefree CLOEXEC
  #  - others  : non-racefree CLOEXEC
  # 
  # @method rd_kafka_conf_set_socket_cb(conf, socket_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] socket_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_socket_cb, :rd_kafka_conf_set_socket_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Set open callback.
  # 
  # The open callback is responsible for opening the file specified by
  # pathname, flags and mode.
  # The file shall be opened with \c CLOEXEC set in a racefree fashion, if
  # possible.
  # 
  # Default:
  #  - on linux: racefree CLOEXEC
  #  - others  : non-racefree CLOEXEC
  # 
  # @method rd_kafka_conf_set_open_cb(conf, open_cb)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*)] open_cb 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_open_cb, :rd_kafka_conf_set_open_cb, [RdKafkaConfS, :pointer], :void
  
  # @brief Sets the application's opaque pointer that will be passed to callbacks
  # 
  # @method rd_kafka_conf_set_opaque(conf, opaque)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*Void)] opaque 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_opaque, :rd_kafka_conf_set_opaque, [RdKafkaConfS, :pointer], :void
  
  # @brief Retrieves the opaque pointer previously set with rd_kafka_conf_set_opaque()
  # 
  # @method rd_kafka_opaque(rk)
  # @param [RdKafkaS] rk 
  # @return [FFI::Pointer(*Void)] 
  # @scope class
  attach_function :rd_kafka_opaque, :rd_kafka_opaque, [RdKafkaS], :pointer
  
  # Sets the default topic configuration to use for automatically
  # subscribed topics (e.g., through pattern-matched topics).
  # The topic config object is not usable after this call.
  # 
  # @method rd_kafka_conf_set_default_topic_conf(conf, tconf)
  # @param [RdKafkaConfS] conf 
  # @param [RdKafkaTopicConfS] tconf 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_set_default_topic_conf, :rd_kafka_conf_set_default_topic_conf, [RdKafkaConfS, RdKafkaTopicConfS], :void
  
  # @brief Retrieve configuration value for property \p name.
  # 
  # If \p dest is non-NULL the value will be written to \p dest with at
  # most \p dest_size.
  # 
  # \p *dest_size is updated to the full length of the value, thus if
  # \p *dest_size initially is smaller than the full length the application
  # may reallocate \p dest to fit the returned \p *dest_size and try again.
  # 
  # If \p dest is NULL only the full length of the value is returned.
  # 
  # Returns \p RD_KAFKA_CONF_OK if the property name matched, else
  # \p RD_KAFKA_CONF_UNKNOWN.
  # 
  # @method rd_kafka_conf_get(conf, name, dest, dest_size)
  # @param [RdKafkaConfS] conf 
  # @param [String] name 
  # @param [String] dest 
  # @param [FFI::Pointer(*SizeT)] dest_size 
  # @return [Symbol from _enum_rd_kafka_conf_res_t_] 
  # @scope class
  attach_function :rd_kafka_conf_get, :rd_kafka_conf_get, [RdKafkaConfS, :string, :string, :pointer], :rd_kafka_conf_res_t
  
  # @brief Retrieve topic configuration value for property \p name.
  # 
  # @sa rd_kafka_conf_get()
  # 
  # @method rd_kafka_topic_conf_get(conf, name, dest, dest_size)
  # @param [RdKafkaTopicConfS] conf 
  # @param [String] name 
  # @param [String] dest 
  # @param [FFI::Pointer(*SizeT)] dest_size 
  # @return [Symbol from _enum_rd_kafka_conf_res_t_] 
  # @scope class
  attach_function :rd_kafka_topic_conf_get, :rd_kafka_topic_conf_get, [RdKafkaTopicConfS, :string, :string, :pointer], :rd_kafka_conf_res_t
  
  # @brief Dump the configuration properties and values of \p conf to an array
  #        with \"key\", \"value\" pairs.
  # 
  # The number of entries in the array is returned in \p *cntp.
  # 
  # The dump must be freed with `rd_kafka_conf_dump_free()`.
  # 
  # @method rd_kafka_conf_dump(conf, cntp)
  # @param [RdKafkaConfS] conf 
  # @param [FFI::Pointer(*SizeT)] cntp 
  # @return [FFI::Pointer(**CharS)] 
  # @scope class
  attach_function :rd_kafka_conf_dump, :rd_kafka_conf_dump, [RdKafkaConfS, :pointer], :pointer
  
  # @brief Dump the topic configuration properties and values of \p conf
  #        to an array with \"key\", \"value\" pairs.
  # 
  # The number of entries in the array is returned in \p *cntp.
  # 
  # The dump must be freed with `rd_kafka_conf_dump_free()`.
  # 
  # @method rd_kafka_topic_conf_dump(conf, cntp)
  # @param [RdKafkaTopicConfS] conf 
  # @param [FFI::Pointer(*SizeT)] cntp 
  # @return [FFI::Pointer(**CharS)] 
  # @scope class
  attach_function :rd_kafka_topic_conf_dump, :rd_kafka_topic_conf_dump, [RdKafkaTopicConfS, :pointer], :pointer
  
  # @brief Frees a configuration dump returned from `rd_kafka_conf_dump()` or
  #        `rd_kafka_topic_conf_dump().
  # 
  # @method rd_kafka_conf_dump_free(arr, cnt)
  # @param [FFI::Pointer(**CharS)] arr 
  # @param [Integer] cnt 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_dump_free, :rd_kafka_conf_dump_free, [:pointer, :ulong], :void
  
  # @brief Prints a table to \p fp of all supported configuration properties,
  #        their default values as well as a description.
  # 
  # @method rd_kafka_conf_properties_show(fp)
  # @param [FFI::Pointer(*FILE)] fp 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_conf_properties_show, :rd_kafka_conf_properties_show, [:pointer], :void
  
  # @brief Create topic configuration object
  # 
  # @sa Same semantics as for rd_kafka_conf_new().
  # 
  # @method rd_kafka_topic_conf_new()
  # @return [RdKafkaTopicConfS] 
  # @scope class
  attach_function :rd_kafka_topic_conf_new, :rd_kafka_topic_conf_new, [], RdKafkaTopicConfS
  
  # @brief Creates a copy/duplicate of topic configuration object \p conf.
  # 
  # @method rd_kafka_topic_conf_dup(conf)
  # @param [RdKafkaTopicConfS] conf 
  # @return [RdKafkaTopicConfS] 
  # @scope class
  attach_function :rd_kafka_topic_conf_dup, :rd_kafka_topic_conf_dup, [RdKafkaTopicConfS], RdKafkaTopicConfS
  
  # @brief Destroys a topic conf object.
  # 
  # @method rd_kafka_topic_conf_destroy(topic_conf)
  # @param [RdKafkaTopicConfS] topic_conf 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_conf_destroy, :rd_kafka_topic_conf_destroy, [RdKafkaTopicConfS], :void
  
  # @brief Sets a single rd_kafka_topic_conf_t value by property name.
  # 
  # \p topic_conf should have been previously set up
  # with `rd_kafka_topic_conf_new()`.
  # 
  # @returns rd_kafka_conf_res_t to indicate success or failure.
  # 
  # @method rd_kafka_topic_conf_set(conf, name, value, errstr, errstr_size)
  # @param [RdKafkaTopicConfS] conf 
  # @param [String] name 
  # @param [String] value 
  # @param [String] errstr 
  # @param [Integer] errstr_size 
  # @return [Symbol from _enum_rd_kafka_conf_res_t_] 
  # @scope class
  attach_function :rd_kafka_topic_conf_set, :rd_kafka_topic_conf_set, [RdKafkaTopicConfS, :string, :string, :string, :ulong], :rd_kafka_conf_res_t
  
  # @brief Sets the application's opaque pointer that will be passed to all topic
  # callbacks as the \c rkt_opaque argument.
  # 
  # @method rd_kafka_topic_conf_set_opaque(conf, opaque)
  # @param [RdKafkaTopicConfS] conf 
  # @param [FFI::Pointer(*Void)] opaque 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_conf_set_opaque, :rd_kafka_topic_conf_set_opaque, [RdKafkaTopicConfS, :pointer], :void
  
  # @brief \b Producer: Set partitioner callback in provided topic conf object.
  # 
  # The partitioner may be called in any thread at any time,
  # it may be called multiple times for the same message/key.
  # 
  # Partitioner function constraints:
  #   - MUST NOT call any rd_kafka_*() functions except:
  #       rd_kafka_topic_partition_available()
  #   - MUST NOT block or execute for prolonged periods of time.
  #   - MUST return a value between 0 and partition_cnt-1, or the
  #     special \c RD_KAFKA_PARTITION_UA value if partitioning
  #     could not be performed.
  # 
  # @method rd_kafka_topic_conf_set_partitioner_cb(topic_conf, partitioner)
  # @param [RdKafkaTopicConfS] topic_conf 
  # @param [FFI::Pointer(*)] partitioner 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_conf_set_partitioner_cb, :rd_kafka_topic_conf_set_partitioner_cb, [RdKafkaTopicConfS, :pointer], :void
  
  # @brief Check if partition is available (has a leader broker).
  # 
  # @returns 1 if the partition is available, else 0.
  # 
  # @warning This function must only be called from inside a partitioner function
  # 
  # @method rd_kafka_topic_partition_available(rkt, partition)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_topic_partition_available, :rd_kafka_topic_partition_available, [RdKafkaTopicS, :int], :int
  
  # @brief Random partitioner.
  # 
  # Will try not to return unavailable partitions.
  # 
  # @returns a random partition between 0 and \p partition_cnt - 1.
  # 
  # @method rd_kafka_msg_partitioner_random(rkt, key, keylen, partition_cnt, opaque, msg_opaque)
  # @param [RdKafkaTopicS] rkt 
  # @param [FFI::Pointer(*Void)] key 
  # @param [Integer] keylen 
  # @param [Integer] partition_cnt 
  # @param [FFI::Pointer(*Void)] opaque 
  # @param [FFI::Pointer(*Void)] msg_opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_msg_partitioner_random, :rd_kafka_msg_partitioner_random, [RdKafkaTopicS, :pointer, :ulong, :int, :pointer, :pointer], :int
  
  # @brief Consistent partitioner.
  # 
  # Uses consistent hashing to map identical keys onto identical partitions.
  # 
  # @returns a \"random\" partition between 0 and partition_cnt - 1 based on
  #          the CRC value of the key
  # 
  # @method rd_kafka_msg_partitioner_consistent(rkt, key, keylen, partition_cnt, opaque, msg_opaque)
  # @param [RdKafkaTopicS] rkt 
  # @param [FFI::Pointer(*Void)] key 
  # @param [Integer] keylen 
  # @param [Integer] partition_cnt 
  # @param [FFI::Pointer(*Void)] opaque 
  # @param [FFI::Pointer(*Void)] msg_opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_msg_partitioner_consistent, :rd_kafka_msg_partitioner_consistent, [RdKafkaTopicS, :pointer, :ulong, :int, :pointer, :pointer], :int
  
  # Consistent-Random partitioner.
  # 
  # This is the default partitioner.
  # Uses consistent hashing to map identical keys onto identical partitions, and
  # messages without keys will be assigned via the random partitioner.
  # 
  # @returns a \"random\" partition between 0 and partition_cnt - 1 based on
  #          the CRC value of the key (if provided)
  # 
  # @method rd_kafka_msg_partitioner_consistent_random(rkt, key, keylen, partition_cnt, opaque, msg_opaque)
  # @param [RdKafkaTopicS] rkt 
  # @param [FFI::Pointer(*Void)] key 
  # @param [Integer] keylen 
  # @param [Integer] partition_cnt 
  # @param [FFI::Pointer(*Void)] opaque 
  # @param [FFI::Pointer(*Void)] msg_opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_msg_partitioner_consistent_random, :rd_kafka_msg_partitioner_consistent_random, [RdKafkaTopicS, :pointer, :ulong, :int, :pointer, :pointer], :int
  
  # @brief Creates a new Kafka handle and starts its operation according to the
  #        specified \p type (\p RD_KAFKA_CONSUMER or \p RD_KAFKA_PRODUCER).
  # 
  # \p conf is an optional struct created with `rd_kafka_conf_new()` that will
  # be used instead of the default configuration.
  # The \p conf object is freed by this function and must not be used or
  # destroyed by the application sub-sequently.
  # See `rd_kafka_conf_set()` et.al for more information.
  # 
  # \p errstr must be a pointer to memory of at least size \p errstr_size where
  # `rd_kafka_new()` may write a human readable error message in case the
  # creation of a new handle fails. In which case the function returns NULL.
  # 
  # @remark \b RD_KAFKA_CONSUMER: When a new \p RD_KAFKA_CONSUMER
  #           rd_kafka_t handle is created it may either operate in the
  #           legacy simple consumer mode using the rd_kafka_consume_start()
  #           interface, or the High-level KafkaConsumer API.
  # @remark An application must only use one of these groups of APIs on a given
  #         rd_kafka_t RD_KAFKA_CONSUMER handle.
  # 
  # 
  # @returns The Kafka handle on success or NULL on error (see \p errstr)
  # 
  # @sa To destroy the Kafka handle, use rd_kafka_destroy().
  # 
  # @method rd_kafka_new(type, conf, errstr, errstr_size)
  # @param [Symbol from _enum_rd_kafka_type_t_] type 
  # @param [RdKafkaConfS] conf 
  # @param [String] errstr 
  # @param [Integer] errstr_size 
  # @return [RdKafkaS] 
  # @scope class
  attach_function :rd_kafka_new, :rd_kafka_new, [:rd_kafka_type_t, RdKafkaConfS, :string, :ulong], RdKafkaS
  
  # @brief Destroy Kafka handle.
  # 
  # @remark This is a blocking operation.
  # 
  # @method rd_kafka_destroy(rk)
  # @param [RdKafkaS] rk 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_destroy, :rd_kafka_destroy, [RdKafkaS], :void
  
  # @brief Returns Kafka handle name.
  # 
  # @method rd_kafka_name(rk)
  # @param [RdKafkaS] rk 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_name, :rd_kafka_name, [RdKafkaS], :string
  
  # @brief Returns this client's broker-assigned group member id 
  # 
  # @remark This currently requires the high-level KafkaConsumer
  # 
  # @returns An allocated string containing the current broker-assigned group
  #          member id, or NULL if not available.
  #          The application must free the string with \p free() or
  #          rd_kafka_mem_free()
  # 
  # @method rd_kafka_memberid(rk)
  # @param [RdKafkaS] rk 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_memberid, :rd_kafka_memberid, [RdKafkaS], :string
  
  # @brief Creates a new topic handle for topic named \p topic.
  # 
  # \p conf is an optional configuration for the topic created with
  # `rd_kafka_topic_conf_new()` that will be used instead of the default
  # topic configuration.
  # The \p conf object is freed by this function and must not be used or
  # destroyed by the application sub-sequently.
  # See `rd_kafka_topic_conf_set()` et.al for more information.
  # 
  # Topic handles are refcounted internally and calling rd_kafka_topic_new()
  # again with the same topic name will return the previous topic handle
  # without updating the original handle's configuration.
  # Applications must eventually call rd_kafka_topic_destroy() for each
  # succesfull call to rd_kafka_topic_new() to clear up resources.
  # 
  # @returns the new topic handle or NULL on error (use rd_kafka_errno2err()
  #          to convert system \p errno to an rd_kafka_resp_err_t error code.
  # 
  # @sa rd_kafka_topic_destroy()
  # 
  # @method rd_kafka_topic_new(rk, topic, conf)
  # @param [RdKafkaS] rk 
  # @param [String] topic 
  # @param [RdKafkaTopicConfS] conf 
  # @return [RdKafkaTopicS] 
  # @scope class
  attach_function :rd_kafka_topic_new, :rd_kafka_topic_new, [RdKafkaS, :string, RdKafkaTopicConfS], RdKafkaTopicS
  
  # @brief Destroy topic handle previously created with `rd_kafka_topic_new()`.
  # 
  # @method rd_kafka_topic_destroy(rkt)
  # @param [RdKafkaTopicS] rkt 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_topic_destroy, :rd_kafka_topic_destroy, [RdKafkaTopicS], :void
  
  # @brief Returns the topic name.
  # 
  # @method rd_kafka_topic_name(rkt)
  # @param [RdKafkaTopicS] rkt 
  # @return [String] 
  # @scope class
  attach_function :rd_kafka_topic_name, :rd_kafka_topic_name, [RdKafkaTopicS], :string
  
  # @brief Get the \p rkt_opaque pointer that was set in the topic configuration.
  # 
  # @method rd_kafka_topic_opaque(rkt)
  # @param [RdKafkaTopicS] rkt 
  # @return [FFI::Pointer(*Void)] 
  # @scope class
  attach_function :rd_kafka_topic_opaque, :rd_kafka_topic_opaque, [RdKafkaTopicS], :pointer
  
  # @brief Polls the provided kafka handle for events.
  # 
  # Events will cause application provided callbacks to be called.
  # 
  # The \p timeout_ms argument specifies the maximum amount of time
  # (in milliseconds) that the call will block waiting for events.
  # For non-blocking calls, provide 0 as \p timeout_ms.
  # To wait indefinately for an event, provide -1.
  # 
  # @remark  An application should make sure to call poll() at regular
  #          intervals to serve any queued callbacks waiting to be called.
  # 
  # Events:
  #   - delivery report callbacks  (if dr_cb/dr_msg_cb is configured) (producer)
  #   - error callbacks (rd_kafka_conf_set_error_cb()) (all)
  #   - stats callbacks (rd_kafka_conf_set_stats_cb()) (all)
  #   - throttle callbacks (rd_kafka_conf_set_throttle_cb()) (all)
  # 
  # @returns the number of events served.
  # 
  # @method rd_kafka_poll(rk, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [Integer] timeout_ms 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_poll, :rd_kafka_poll, [RdKafkaS, :int], :int
  
  # @brief Cancels the current callback dispatcher (rd_kafka_poll(),
  #        rd_kafka_consume_callback(), etc).
  # 
  # A callback may use this to force an immediate return to the calling
  # code (caller of e.g. rd_kafka_poll()) without processing any further
  # events.
  # 
  # @remark This function MUST ONLY be called from within a librdkafka callback.
  # 
  # @method rd_kafka_yield(rk)
  # @param [RdKafkaS] rk 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_yield, :rd_kafka_yield, [RdKafkaS], :void
  
  # @brief Pause producing or consumption for the provided list of partitions.
  # 
  # Success or error is returned per-partition \p err in the \p partitions list.
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR
  # 
  # @method rd_kafka_pause_partitions(rk, partitions)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] partitions 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_pause_partitions, :rd_kafka_pause_partitions, [RdKafkaS, RdKafkaTopicPartitionListS], :rd_kafka_resp_err_t
  
  # @brief Resume producing consumption for the provided list of partitions.
  # 
  # Success or error is returned per-partition \p err in the \p partitions list.
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR
  # 
  # @method rd_kafka_resume_partitions(rk, partitions)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] partitions 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_resume_partitions, :rd_kafka_resume_partitions, [RdKafkaS, RdKafkaTopicPartitionListS], :rd_kafka_resp_err_t
  
  # @brief Get low (oldest/beginning) and high (newest/end) offsets
  #        for partition.
  # 
  # Offsets are returned in \p *low and \p *high respectively.
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on failure.
  # 
  # @method rd_kafka_get_offsets(rk, topic, partition, low, high, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [String] topic 
  # @param [Integer] partition 
  # @param [FFI::Pointer(*Int64T)] low 
  # @param [FFI::Pointer(*Int64T)] high 
  # @param [Integer] timeout_ms 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_get_offsets, :rd_kafka_get_offsets, [RdKafkaS, :string, :int, :pointer, :pointer, :int], :rd_kafka_resp_err_t
  
  # @brief Free pointer returned by librdkafka
  # 
  # This is typically an abstraction for the free(3) call and makes sure
  # the application can use the same memory allocator as librdkafka for
  # freeing pointers returned by librdkafka.
  # 
  # In standard setups it is usually not necessary to use this interface
  # rather than the free(3) functione.
  # 
  # @remark rd_kafka_mem_free() must only be used for pointers returned by APIs
  #         that explicitly mention using this function for freeing.
  # 
  # @method rd_kafka_mem_free(rk, ptr)
  # @param [RdKafkaS] rk 
  # @param [FFI::Pointer(*Void)] ptr 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_mem_free, :rd_kafka_mem_free, [RdKafkaS, :pointer], :void
  
  # @brief Create a new message queue.
  # 
  # See rd_kafka_consume_start_queue(), rd_kafka_consume_queue(), et.al.
  # 
  # @method rd_kafka_queue_new(rk)
  # @param [RdKafkaS] rk 
  # @return [RdKafkaQueueS] 
  # @scope class
  attach_function :rd_kafka_queue_new, :rd_kafka_queue_new, [RdKafkaS], RdKafkaQueueS
  
  # Destroy a queue, purging all of its enqueued messages.
  # 
  # @method rd_kafka_queue_destroy(rkqu)
  # @param [RdKafkaQueueS] rkqu 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_queue_destroy, :rd_kafka_queue_destroy, [RdKafkaQueueS], :void
  
  # @brief Start consuming messages for topic \p rkt and \p partition
  # at offset \p offset which may either be an absolute \c (0..N)
  # or one of the logical offsets:
  #  - RD_KAFKA_OFFSET_BEGINNING
  #  - RD_KAFKA_OFFSET_END
  #  - RD_KAFKA_OFFSET_STORED
  #  - RD_KAFKA_OFFSET_TAIL
  # 
  # rdkafka will attempt to keep \c queued.min.messages (config property)
  # messages in the local queue by repeatedly fetching batches of messages
  # from the broker until the threshold is reached.
  # 
  # The application shall use one of the `rd_kafka_consume*()` functions
  # to consume messages from the local queue, each kafka message being
  # represented as a `rd_kafka_message_t *` object.
  # 
  # `rd_kafka_consume_start()` must not be called multiple times for the same
  # topic and partition without stopping consumption first with
  # `rd_kafka_consume_stop()`.
  # 
  # @returns 0 on success or -1 on error in which case errno is set accordingly:
  #  - EBUSY    - Conflicts with an existing or previous subscription
  #               (RD_KAFKA_RESP_ERR__CONFLICT)
  #  - EINVAL   - Invalid offset, or incomplete configuration (lacking group.id)
  #               (RD_KAFKA_RESP_ERR__INVALID_ARG)
  #  - ESRCH    - requested \p partition is invalid.
  #               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
  #  - ENOENT   - topic is unknown in the Kafka cluster.
  #               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
  # 
  # Use `rd_kafka_errno2err()` to convert sytem \c errno to `rd_kafka_resp_err_t`
  # 
  # @method rd_kafka_consume_start(rkt, partition, offset)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] offset 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_start, :rd_kafka_consume_start, [RdKafkaTopicS, :int, :long_long], :int
  
  # @brief Same as rd_kafka_consume_start() but re-routes incoming messages to
  # the provided queue \p rkqu (which must have been previously allocated
  # with `rd_kafka_queue_new()`.
  # 
  # The application must use one of the `rd_kafka_consume_*_queue()` functions
  # to receive fetched messages.
  # 
  # `rd_kafka_consume_start_queue()` must not be called multiple times for the
  # same topic and partition without stopping consumption first with
  # `rd_kafka_consume_stop()`.
  # `rd_kafka_consume_start()` and `rd_kafka_consume_start_queue()` must not
  # be combined for the same topic and partition.
  # 
  # @method rd_kafka_consume_start_queue(rkt, partition, offset, rkqu)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] offset 
  # @param [RdKafkaQueueS] rkqu 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_start_queue, :rd_kafka_consume_start_queue, [RdKafkaTopicS, :int, :long_long, RdKafkaQueueS], :int
  
  # @brief Stop consuming messages for topic \p rkt and \p partition, purging
  # all messages currently in the local queue.
  # 
  # NOTE: To enforce synchronisation this call will block until the internal
  #       fetcher has terminated and offsets are commited to configured
  #       storage method.
  # 
  # The application needs to be stop all consumers before calling
  # `rd_kafka_destroy()` on the main object handle.
  # 
  # @returns 0 on success or -1 on error (see `errno`).
  # 
  # @method rd_kafka_consume_stop(rkt, partition)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_stop, :rd_kafka_consume_stop, [RdKafkaTopicS, :int], :int
  
  # @brief Seek consumer for topic+partition to \p offset which is either an
  #        absolute or logical offset.
  # 
  # If \p timeout_ms is not 0 the call will wait this long for the
  # seek to be performed. If the timeout is reached the internal state
  # will be unknown and this function returns `RD_KAFKA_RESP_ERR__TIMED_OUT`.
  # If \p timeout_ms is 0 it will initiate the seek but return
  # immediately without any error reporting (e.g., async).
  # 
  # This call triggers a fetch queue barrier flush.
  # 
  # @returns `RD_KAFKA_RESP_ERR__NO_ERROR` on success else an error code.
  # 
  # @method rd_kafka_seek(rkt, partition, offset, timeout_ms)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] offset 
  # @param [Integer] timeout_ms 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_seek, :rd_kafka_seek, [RdKafkaTopicS, :int, :long_long, :int], :rd_kafka_resp_err_t
  
  # @brief Consume a single message from topic \p rkt and \p partition
  # 
  # \p timeout_ms is maximum amount of time to wait for a message to be received.
  # Consumer must have been previously started with `rd_kafka_consume_start()`.
  # 
  # Returns a message object on success or \c NULL on error.
  # The message object must be destroyed with `rd_kafka_message_destroy()`
  # when the application is done with it.
  # 
  # Errors (when returning NULL):
  #  - ETIMEDOUT - \p timeout_ms was reached with no new messages fetched.
  #  - ENOENT    - \p rkt + \p partition is unknown.
  #                 (no prior `rd_kafka_consume_start()` call)
  # 
  # NOTE: The returned message's \c ..->err must be checked for errors.
  # NOTE: \c ..->err \c == \c RD_KAFKA_RESP_ERR__PARTITION_EOF signals that the
  #       end of the partition has been reached, which should typically not be
  #       considered an error. The application should handle this case
  #       (e.g., ignore).
  # 
  # @method rd_kafka_consume(rkt, partition, timeout_ms)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] timeout_ms 
  # @return [RdKafkaMessageS] 
  # @scope class
  attach_function :rd_kafka_consume, :rd_kafka_consume, [RdKafkaTopicS, :int, :int], RdKafkaMessageS
  
  # @brief Consume up to \p rkmessages_size from topic \p rkt and \p partition
  #        putting a pointer to each message in the application provided
  #        array \p rkmessages (of size \p rkmessages_size entries).
  # 
  # `rd_kafka_consume_batch()` provides higher throughput performance
  # than `rd_kafka_consume()`.
  # 
  # \p timeout_ms is the maximum amount of time to wait for all of
  # \p rkmessages_size messages to be put into \p rkmessages.
  # If no messages were available within the timeout period this function
  # returns 0 and \p rkmessages remains untouched.
  # This differs somewhat from `rd_kafka_consume()`.
  # 
  # The message objects must be destroyed with `rd_kafka_message_destroy()`
  # when the application is done with it.
  # 
  # @returns the number of rkmessages added in \p rkmessages,
  # or -1 on error (same error codes as for `rd_kafka_consume()`.
  # 
  # @sa rd_kafka_consume()
  # 
  # @method rd_kafka_consume_batch(rkt, partition, timeout_ms, rkmessages, rkmessages_size)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] timeout_ms 
  # @param [FFI::Pointer(**RdKafkaMessageT)] rkmessages 
  # @param [Integer] rkmessages_size 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_batch, :rd_kafka_consume_batch, [RdKafkaTopicS, :int, :int, :pointer, :ulong], :long
  
  # @brief Consumes messages from topic \p rkt and \p partition, calling
  # the provided callback for each consumed messsage.
  # 
  # `rd_kafka_consume_callback()` provides higher throughput performance
  # than both `rd_kafka_consume()` and `rd_kafka_consume_batch()`.
  # 
  # \p timeout_ms is the maximum amount of time to wait for one or more messages
  # to arrive.
  # 
  # The provided \p consume_cb function is called for each message,
  # the application \b MUST \b NOT call `rd_kafka_message_destroy()` on the
  # provided \p rkmessage.
  # 
  # The \p opaque argument is passed to the 'consume_cb' as \p opaque.
  # 
  # @returns the number of messages processed or -1 on error.
  # 
  # @sa rd_kafka_consume()
  # 
  # @method rd_kafka_consume_callback(rkt, partition, timeout_ms, consume_cb, opaque)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] timeout_ms 
  # @param [FFI::Pointer(*)] consume_cb 
  # @param [FFI::Pointer(*Void)] opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_callback, :rd_kafka_consume_callback, [RdKafkaTopicS, :int, :int, :pointer, :pointer], :int
  
  # @brief Consume from queue
  # 
  # @sa rd_kafka_consume()
  # 
  # @method rd_kafka_consume_queue(rkqu, timeout_ms)
  # @param [RdKafkaQueueS] rkqu 
  # @param [Integer] timeout_ms 
  # @return [RdKafkaMessageS] 
  # @scope class
  attach_function :rd_kafka_consume_queue, :rd_kafka_consume_queue, [RdKafkaQueueS, :int], RdKafkaMessageS
  
  # @brief Consume batch of messages from queue
  # 
  # @sa rd_kafka_consume_batch()
  # 
  # @method rd_kafka_consume_batch_queue(rkqu, timeout_ms, rkmessages, rkmessages_size)
  # @param [RdKafkaQueueS] rkqu 
  # @param [Integer] timeout_ms 
  # @param [FFI::Pointer(**RdKafkaMessageT)] rkmessages 
  # @param [Integer] rkmessages_size 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_batch_queue, :rd_kafka_consume_batch_queue, [RdKafkaQueueS, :int, :pointer, :ulong], :long
  
  # @brief Consume multiple messages from queue with callback
  # 
  # @sa rd_kafka_consume_callback()
  # 
  # @method rd_kafka_consume_callback_queue(rkqu, timeout_ms, consume_cb, opaque)
  # @param [RdKafkaQueueS] rkqu 
  # @param [Integer] timeout_ms 
  # @param [FFI::Pointer(*)] consume_cb 
  # @param [FFI::Pointer(*Void)] opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_consume_callback_queue, :rd_kafka_consume_callback_queue, [RdKafkaQueueS, :int, :pointer, :pointer], :int
  
  # @brief Store offset \p offset for topic \p rkt partition \p partition.
  # 
  # The offset will be commited (written) to the offset store according
  # to \c `auto.commit.interval.ms`.
  # 
  # @remark \c `auto.commit.enable` must be set to "false" when using this API.
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
  # 
  # @method rd_kafka_offset_store(rkt, partition, offset)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] offset 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_offset_store, :rd_kafka_offset_store, [RdKafkaTopicS, :int, :long_long], :rd_kafka_resp_err_t
  
  # @brief Subscribe to topic set using balanced consumer groups.
  # 
  # Wildcard (regex) topics are supported by the librdkafka assignor:
  # any topic name in the \p topics list that is prefixed with \c \"^\" will
  # be regex-matched to the full list of topics in the cluster and matching
  # topics will be added to the subscription list.
  # 
  # @method rd_kafka_subscribe(rk, topics)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] topics 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_subscribe, :rd_kafka_subscribe, [RdKafkaS, RdKafkaTopicPartitionListS], :rd_kafka_resp_err_t
  
  # @brief Unsubscribe from the current subscription set.
  # 
  # @method rd_kafka_unsubscribe(rk)
  # @param [RdKafkaS] rk 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_unsubscribe, :rd_kafka_unsubscribe, [RdKafkaS], :rd_kafka_resp_err_t
  
  # @brief Returns the current topic subscription
  # 
  # @returns An error code on failure, otherwise \p topic is updated
  #          to point to a newly allocated topic list (possibly empty).
  # 
  # @remark The application is responsible for calling
  #         rd_kafka_topic_partition_list_destroy on the returned list.
  # 
  # @method rd_kafka_subscription(rk, topics)
  # @param [RdKafkaS] rk 
  # @param [FFI::Pointer(**RdKafkaTopicPartitionListT)] topics 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_subscription, :rd_kafka_subscription, [RdKafkaS, :pointer], :rd_kafka_resp_err_t
  
  # @brief Poll the consumer for messages or events.
  # 
  # Will block for at most \p timeout_ms milliseconds.
  # 
  # @remark  An application should make sure to call consumer_poll() at regular
  #          intervals, even if no messages are expected, to serve any
  #          queued callbacks waiting to be called. This is especially
  #          important when a rebalance_cb has been registered as it needs
  #          to be called and handled properly to synchronize internal
  #          consumer state.
  # 
  # @returns A message object which is a proper message if \p ->err is
  #          RD_KAFKA_RESP_ERR_NO_ERROR, or an event or error for any other
  #          value.
  # 
  # @sa rd_kafka_message_t
  # 
  # @method rd_kafka_consumer_poll(rk, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [Integer] timeout_ms 
  # @return [RdKafkaMessageS] 
  # @scope class
  attach_function :rd_kafka_consumer_poll, :rd_kafka_consumer_poll, [RdKafkaS, :int], RdKafkaMessageS
  
  # @brief Close down the KafkaConsumer.
  # 
  # @remark This call will block until the consumer has revoked its assignment,
  #         calling the \c rebalance_cb if it is configured, committed offsets
  #         to broker, and left the consumer group.
  # 
  # @returns An error code indicating if the consumer close was succesful
  #          or not.
  # 
  # @remark The application still needs to call rd_kafka_destroy() after
  #         this call finishes to clean up the underlying handle resources.
  # 
  # @method rd_kafka_consumer_close(rk)
  # @param [RdKafkaS] rk 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_consumer_close, :rd_kafka_consumer_close, [RdKafkaS], :rd_kafka_resp_err_t
  
  # @brief Atomic assignment of partitions to consume.
  # 
  # @method rd_kafka_assign(rk, partitions)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] partitions 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_assign, :rd_kafka_assign, [RdKafkaS, RdKafkaTopicPartitionListS], :rd_kafka_resp_err_t
  
  # @brief Returns the current partition assignment
  # 
  # @returns An error code on failure, otherwise \p partitions is updated
  #          to point to a newly allocated partition list (possibly empty).
  # 
  # @remark The application is responsible for calling
  #         rd_kafka_topic_partition_list_destroy on the returned list.
  # 
  # @method rd_kafka_assignment(rk, partitions)
  # @param [RdKafkaS] rk 
  # @param [FFI::Pointer(**RdKafkaTopicPartitionListT)] partitions 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_assignment, :rd_kafka_assignment, [RdKafkaS, :pointer], :rd_kafka_resp_err_t
  
  # @brief Commit offsets on broker for the provided list of partitions.
  # 
  # \p offsets should contain \c topic, \c partition, \c offset and possibly
  # \c metadata.
  # If \p offsets is NULL the current partition assignment will be used instead.
  # 
  # If \p async is false this operation will block until the broker offset commit
  # is done, returning the resulting success or error code.
  # 
  # If a rd_kafka_conf_set_offset_commit_cb() offset commit callback has been
  # configured a callback will be enqueued for a future call to rd_kafka_poll().
  # 
  # @method rd_kafka_commit(rk, offsets, async)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] offsets 
  # @param [Integer] async 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_commit, :rd_kafka_commit, [RdKafkaS, RdKafkaTopicPartitionListS, :int], :rd_kafka_resp_err_t
  
  # @brief Commit message's offset on broker for the message's partition.
  # 
  # @method rd_kafka_commit_message(rk, rkmessage, async)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaMessageS] rkmessage 
  # @param [Integer] async 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_commit_message, :rd_kafka_commit_message, [RdKafkaS, RdKafkaMessageS, :int], :rd_kafka_resp_err_t
  
  # @brief Retrieve committed positions (offsets) for topics+partitions.
  # 
  # @returns RD_KAFKA_RESP_ERR_NO_ERROR on success in which case the
  #          \p offset or \p err field of each \p partitions' element is filled
  #          in with the stored offset, or a partition specific error.
  #          Else returns an error code.
  # 
  # @method rd_kafka_position(rk, partitions, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [RdKafkaTopicPartitionListS] partitions 
  # @param [Integer] timeout_ms 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_position, :rd_kafka_position, [RdKafkaS, RdKafkaTopicPartitionListS, :int], :rd_kafka_resp_err_t
  
  # @brief Produce and send a single message to broker.
  # 
  # \p rkt is the target topic which must have been previously created with
  # `rd_kafka_topic_new()`.
  # 
  # `rd_kafka_produce()` is an asynch non-blocking API.
  # 
  # \p partition is the target partition, either:
  #   - RD_KAFKA_PARTITION_UA (unassigned) for
  #     automatic partitioning using the topic's partitioner function, or
  #   - a fixed partition (0..N)
  # 
  # \p msgflags is zero or more of the following flags OR:ed together:
  #    RD_KAFKA_MSG_F_FREE - rdkafka will free(3) \p payload when it is done
  #                          with it.
  #    RD_KAFKA_MSG_F_COPY - the \p payload data will be copied and the 
  #                          \p payload pointer will not be used by rdkafka
  #                          after the call returns.
  # 
  #    .._F_FREE and .._F_COPY are mutually exclusive.
  # 
  #    If the function returns -1 and RD_KAFKA_MSG_F_FREE was specified, then
  #    the memory associated with the payload is still the caller's
  #    responsibility.
  # 
  # \p payload is the message payload of size \p len bytes.
  # 
  # \p key is an optional message key of size \p keylen bytes, if non-NULL it
  # will be passed to the topic partitioner as well as be sent with the
  # message to the broker and passed on to the consumer.
  # 
  # \p msg_opaque is an optional application-provided per-message opaque
  # pointer that will provided in the delivery report callback (`dr_cb`) for
  # referencing this message.
  # 
  # Returns 0 on success or -1 on error in which case errno is set accordingly:
  #  - ENOBUFS  - maximum number of outstanding messages has been reached:
  #               "queue.buffering.max.messages"
  #               (RD_KAFKA_RESP_ERR__QUEUE_FULL)
  #  - EMSGSIZE - message is larger than configured max size:
  #               "messages.max.bytes".
  #               (RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE)
  #  - ESRCH    - requested \p partition is unknown in the Kafka cluster.
  #               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
  #  - ENOENT   - topic is unknown in the Kafka cluster.
  #               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
  # 
  # @sa Use rd_kafka_errno2err() to convert `errno` to rdkafka error code.
  # 
  # @method rd_kafka_produce(rkt, partitition, msgflags, payload, len, key, keylen, msg_opaque)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partitition 
  # @param [Integer] msgflags 
  # @param [FFI::Pointer(*Void)] payload 
  # @param [Integer] len 
  # @param [FFI::Pointer(*Void)] key 
  # @param [Integer] keylen 
  # @param [FFI::Pointer(*Void)] msg_opaque 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_produce, :rd_kafka_produce, [RdKafkaTopicS, :int, :int, :pointer, :ulong, :pointer, :ulong, :pointer], :int
  
  # @brief Produce multiple messages.
  # 
  # If partition is RD_KAFKA_PARTITION_UA the configured partitioner will
  # be run for each message (slower), otherwise the messages will be enqueued
  # to the specified partition directly (faster).
  # 
  # The messages are provided in the array \p rkmessages of count \p message_cnt
  # elements.
  # The \p partition and \p msgflags are used for all provided messages.
  # 
  # Honoured \p rkmessages() fields are:
  #  - payload,len    Message payload and length
  #  - key,key_len    Optional message key
  #  - _private       Message opaque pointer (msg_opaque)
  #  - err            Will be set according to success or failure.
  #                   Application only needs to check for errors if
  #                   return value != \p message_cnt.
  # 
  # @returns the number of messages succesfully enqueued for producing.
  # 
  # @method rd_kafka_produce_batch(rkt, partition, msgflags, rkmessages, message_cnt)
  # @param [RdKafkaTopicS] rkt 
  # @param [Integer] partition 
  # @param [Integer] msgflags 
  # @param [RdKafkaMessageS] rkmessages 
  # @param [Integer] message_cnt 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_produce_batch, :rd_kafka_produce_batch, [RdKafkaTopicS, :int, :int, RdKafkaMessageS, :int], :int
  
  # @brief Broker information
  # 
  # = Fields:
  # :id ::
  #   (Integer) < Broker Id
  # :host ::
  #   (String) < Broker hostname
  # :port ::
  #   (Integer) < Broker listening port
  class RdKafkaMetadataBroker < FFI::Struct
    layout :id, :int,
           :host, :string,
           :port, :int
  end
  
  # @brief Partition information
  # 
  # = Fields:
  # :id ::
  #   (Integer) < Partition Id
  # :err ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Partition error reported by broker
  # :leader ::
  #   (Integer) < Leader broker
  # :replica_cnt ::
  #   (Integer) < Number of brokers in \p replicas
  # :replicas ::
  #   (FFI::Pointer(*Int32T)) < Replica brokers
  # :isr_cnt ::
  #   (Integer) < Number of ISR brokers in \p isrs
  # :isrs ::
  #   (FFI::Pointer(*Int32T)) < In-Sync-Replica brokers
  class RdKafkaMetadataPartition < FFI::Struct
    layout :id, :int,
           :err, :rd_kafka_resp_err_t,
           :leader, :int,
           :replica_cnt, :int,
           :replicas, :pointer,
           :isr_cnt, :int,
           :isrs, :pointer
  end
  
  # @brief Topic information
  # 
  # = Fields:
  # :topic ::
  #   (String) < Topic name
  # :partition_cnt ::
  #   (Integer) < Number of partitions in \p partitions
  # :partitions ::
  #   (RdKafkaMetadataPartition) < Partitions
  # :err ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Topic error reported by broker
  class RdKafkaMetadataTopic < FFI::Struct
    layout :topic, :string,
           :partition_cnt, :int,
           :partitions, RdKafkaMetadataPartition,
           :err, :rd_kafka_resp_err_t
  end
  
  # @brief Metadata container
  # 
  # = Fields:
  # :broker_cnt ::
  #   (Integer) < Number of brokers in \p brokers
  # :brokers ::
  #   (RdKafkaMetadataBroker) < Brokers
  # :topic_cnt ::
  #   (Integer) < Number of topics in \p topics
  # :topics ::
  #   (RdKafkaMetadataTopic) < Topics
  # :orig_broker_id ::
  #   (Integer) < Broker originating this metadata
  # :orig_broker_name ::
  #   (String) < Name of originating broker
  module RdKafkaMetadataWrappers
    # @return [nil] 
    def destroy()
      RbKafka.rd_kafka_metadata_destroy(self)
    end
  end
  
  class RdKafkaMetadata < FFI::Struct
    include RdKafkaMetadataWrappers
    layout :broker_cnt, :int,
           :brokers, RdKafkaMetadataBroker,
           :topic_cnt, :int,
           :topics, RdKafkaMetadataTopic,
           :orig_broker_id, :int,
           :orig_broker_name, :string
  end
  
  # @brief Request Metadata from broker.
  # 
  # Parameters:
  #  - \p all_topics  if non-zero: request info about all topics in cluster,
  #                   if zero: only request info about locally known topics.
  #  - \p only_rkt    only request info about this topic
  #  - \p metadatap   pointer to hold metadata result.
  #                   The \p *metadatap pointer must be released
  #                   with rd_kafka_metadata_destroy().
  #  - \p timeout_ms  maximum response time before failing.
  # 
  # Returns RD_KAFKA_RESP_ERR_NO_ERROR on success (in which case *metadatap)
  # will be set, else RD_KAFKA_RESP_ERR__TIMED_OUT on timeout or
  # other error code on error.
  # 
  # @method rd_kafka_metadata(rk, all_topics, only_rkt, metadatap, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [Integer] all_topics 
  # @param [RdKafkaTopicS] only_rkt 
  # @param [FFI::Pointer(**RdKafkaMetadata)] metadatap 
  # @param [Integer] timeout_ms 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_metadata, :rd_kafka_metadata, [RdKafkaS, :int, RdKafkaTopicS, :pointer, :int], :rd_kafka_resp_err_t
  
  # @brief Release metadata memory.
  # 
  # @method rd_kafka_metadata_destroy(metadata)
  # @param [RdKafkaMetadata] metadata 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_metadata_destroy, :rd_kafka_metadata_destroy, [RdKafkaMetadata], :void
  
  # @brief Group member information
  # 
  # For more information on \p member_metadata format, see
  # https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-GroupMembershipAPI
  # 
  # = Fields:
  # :member_id ::
  #   (String) < Member id (generated by broker)
  # :client_id ::
  #   (String) < Client's \p client.id
  # :client_host ::
  #   (String) < Client's hostname
  # :member_metadata ::
  #   (FFI::Pointer(*Void)) < Member metadata (binary),
  #     format depends on \p protocol_type.
  # :member_metadata_size ::
  #   (Integer) < Member metadata size in bytes
  # :member_assignment ::
  #   (FFI::Pointer(*Void)) < Member assignment (binary),
  #      format depends on \p protocol_type.
  # :member_assignment_size ::
  #   (Integer) < Member assignment size in bytes
  class RdKafkaGroupMemberInfo < FFI::Struct
    layout :member_id, :string,
           :client_id, :string,
           :client_host, :string,
           :member_metadata, :pointer,
           :member_metadata_size, :int,
           :member_assignment, :pointer,
           :member_assignment_size, :int
  end
  
  # @brief Group information
  # 
  # = Fields:
  # :broker ::
  #   (RdKafkaMetadataBroker) < Originating broker info
  # :group ::
  #   (String) < Group name
  # :err ::
  #   (Symbol from _enum_rd_kafka_resp_err_t_) < Broker-originated error
  # :state ::
  #   (String) < Group state
  # :protocol_type ::
  #   (String) < Group protocol type
  # :protocol ::
  #   (String) < Group protocol
  # :members ::
  #   (RdKafkaGroupMemberInfo) < Group members
  # :member_cnt ::
  #   (Integer) < Group member count
  class RdKafkaGroupInfo < FFI::Struct
    layout :broker, RdKafkaMetadataBroker.by_value,
           :group, :string,
           :err, :rd_kafka_resp_err_t,
           :state, :string,
           :protocol_type, :string,
           :protocol, :string,
           :members, RdKafkaGroupMemberInfo,
           :member_cnt, :int
  end
  
  # @brief List of groups
  # 
  # @sa rd_kafka_group_list_destroy() to release list memory.
  # 
  # = Fields:
  # :groups ::
  #   (RdKafkaGroupInfo) < Groups
  # :group_cnt ::
  #   (Integer) < Group count
  module RdKafkaGroupListWrappers
    # @return [nil] 
    def destroy()
      RbKafka.rd_kafka_group_list_destroy(self)
    end
  end
  
  class RdKafkaGroupList < FFI::Struct
    include RdKafkaGroupListWrappers
    layout :groups, RdKafkaGroupInfo,
           :group_cnt, :int
  end
  
  # @brief List and describe client groups in cluster.
  # 
  # \p group is an optional group name to describe, otherwise (\p NULL) all
  # groups are returned.
  # 
  # \p timeout_ms is the (approximate) maximum time to wait for response
  # from brokers and must be a positive value.
  # 
  # @returns \p RD_KAFKA_RESP_ERR__NO_ERROR on success and \p grplistp is
  #           updated to point to a newly allocated list of groups.
  #           Else returns an error code on failure and \p grplistp remains
  #           untouched.
  # 
  # @sa Use rd_kafka_group_list_destroy() to release list memory.
  # 
  # @method rd_kafka_list_groups(rk, group, grplistp, timeout_ms)
  # @param [RdKafkaS] rk 
  # @param [String] group 
  # @param [FFI::Pointer(**RdKafkaGroupList)] grplistp 
  # @param [Integer] timeout_ms 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_list_groups, :rd_kafka_list_groups, [RdKafkaS, :string, :pointer, :int], :rd_kafka_resp_err_t
  
  # @brief Release list memory
  # 
  # @method rd_kafka_group_list_destroy(grplist)
  # @param [RdKafkaGroupList] grplist 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_group_list_destroy, :rd_kafka_group_list_destroy, [RdKafkaGroupList], :void
  
  # @brief Adds one or more brokers to the kafka handle's list of initial
  #        bootstrap brokers.
  # 
  # Additional brokers will be discovered automatically as soon as rdkafka
  # connects to a broker by querying the broker metadata.
  # 
  # If a broker name resolves to multiple addresses (and possibly
  # address families) all will be used for connection attempts in
  # round-robin fashion.
  # 
  # \p brokerlist is a ,-separated list of brokers in the format:
  #   \c \<broker1\>,\<broker2\>,..
  # Where each broker is in either the host or URL based format:
  #   \c \<host\>(:\<port\>)
  #   \c \<proto\>://\<host\>(:port)
  # \c \<proto\> is either \c PLAINTEXT, \c SSL, \c SASL, \c SASL_PLAINTEXT
  # The two formats can be mixed but ultimately the value of the
  # `security.protocol` config property decides what brokers are allowed.
  # 
  # Example:
  #    brokerlist = "broker1:10000,broker2"
  #    brokerlist = "SSL://broker3:9000,ssl://broker2"
  # 
  # @returns the number of brokers successfully added.
  # 
  # @remark Brokers may also be defined with the \c metadata.broker.list or
  #         \c bootstrap.servers configuration property (preferred method).
  # 
  # @method rd_kafka_brokers_add(rk, brokerlist)
  # @param [RdKafkaS] rk 
  # @param [String] brokerlist 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_brokers_add, :rd_kafka_brokers_add, [RdKafkaS, :string], :int
  
  # @brief Set logger function.
  # 
  # The default is to print to stderr, but a syslog logger is also available,
  # see rd_kafka_log_(print|syslog) for the builtin alternatives.
  # Alternatively the application may provide its own logger callback.
  # Or pass 'func' as NULL to disable logging.
  # 
  # @deprecated Use rd_kafka_conf_set_log_cb()
  # 
  # @remark \p rk may be passed as NULL in the callback.
  # 
  # @method rd_kafka_set_logger(rk, func)
  # @param [RdKafkaS] rk 
  # @param [FFI::Pointer(*)] func 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_set_logger, :rd_kafka_set_logger, [RdKafkaS, :pointer], :void
  
  # @brief Specifies the maximum logging level produced by
  #        internal kafka logging and debugging.
  # 
  # If the \p \"debug\" configuration property is set the level is automatically
  # adjusted to \c LOG_DEBUG (7).
  # 
  # @method rd_kafka_set_log_level(rk, level)
  # @param [RdKafkaS] rk 
  # @param [Integer] level 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_set_log_level, :rd_kafka_set_log_level, [RdKafkaS, :int], :void
  
  # @brief Builtin (default) log sink: print to stderr
  # 
  # @method rd_kafka_log_print(rk, level, fac, buf)
  # @param [RdKafkaS] rk 
  # @param [Integer] level 
  # @param [String] fac 
  # @param [String] buf 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_log_print, :rd_kafka_log_print, [RdKafkaS, :int, :string, :string], :void
  
  # @brief Builtin log sink: print to syslog.
  # 
  # @method rd_kafka_log_syslog(rk, level, fac, buf)
  # @param [RdKafkaS] rk 
  # @param [Integer] level 
  # @param [String] fac 
  # @param [String] buf 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_log_syslog, :rd_kafka_log_syslog, [RdKafkaS, :int, :string, :string], :void
  
  # @brief Returns the current out queue length.
  # 
  # The out queue contains messages waiting to be sent to, or acknowledged by,
  # the broker.
  # 
  # An application should wait for this queue to reach zero before terminating
  # to make sure outstanding requests (such as offset commits) are fully
  # processed.
  # 
  # @returns number of messages in the out queue.
  # 
  # @method rd_kafka_outq_len(rk)
  # @param [RdKafkaS] rk 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_outq_len, :rd_kafka_outq_len, [RdKafkaS], :int
  
  # @brief Dumps rdkafka's internal state for handle \p rk to stream \p fp
  # 
  # This is only useful for debugging rdkafka, showing state and statistics
  # for brokers, topics, partitions, etc.
  # 
  # @method rd_kafka_dump(fp, rk)
  # @param [FFI::Pointer(*FILE)] fp 
  # @param [RdKafkaS] rk 
  # @return [nil] 
  # @scope class
  attach_function :rd_kafka_dump, :rd_kafka_dump, [:pointer, RdKafkaS], :void
  
  # @brief Retrieve the current number of threads in use by librdkafka.
  # 
  # Used by regression tests.
  # 
  # @method rd_kafka_thread_cnt()
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_thread_cnt, :rd_kafka_thread_cnt, [], :int
  
  # @brief Wait for all rd_kafka_t objects to be destroyed.
  # 
  # Returns 0 if all kafka objects are now destroyed, or -1 if the
  # timeout was reached.
  # Since `rd_kafka_destroy()` is an asynch operation the 
  # `rd_kafka_wait_destroyed()` function can be used for applications where
  # a clean shutdown is required.
  # 
  # @method rd_kafka_wait_destroyed(timeout_ms)
  # @param [Integer] timeout_ms 
  # @return [Integer] 
  # @scope class
  attach_function :rd_kafka_wait_destroyed, :rd_kafka_wait_destroyed, [:int], :int
  
  # @brief Redirect the main (rd_kafka_poll()) queue to the KafkaConsumer's
  #        queue (rd_kafka_consumer_poll()).
  # 
  # @warning It is not permitted to call rd_kafka_poll() after directing the
  #          main queue with rd_kafka_poll_set_consumer().
  # 
  # @method rd_kafka_poll_set_consumer(rk)
  # @param [RdKafkaS] rk 
  # @return [Symbol from _enum_rd_kafka_resp_err_t_] 
  # @scope class
  attach_function :rd_kafka_poll_set_consumer, :rd_kafka_poll_set_consumer, [RdKafkaS], :rd_kafka_resp_err_t
  
end
